{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at aitextgen/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen \n",
    "\n",
    "ai = aitextgen()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couples often have a good relationship. These are often the two most important things in marriage. And if they're not, it's important to have a healthy relationship.\n",
      "\n",
      "Let's get back to our love.\n",
      "\n",
      "You know, I'm going to call you Dad. And you know what Dad is? Well, you are totally right. I'm about to tell you what I think. And you will be surprised to learn that I am actually on a mission.\n",
      "\n",
      "I'm going to tell you what I think.\n",
      "\n",
      "Let's start with our first love. The one who has always called me Dad.\n",
      "\n",
      "I have two things in common.\n",
      "\n",
      "First, I'm a very loyal, kind, and loving husband.\n",
      "\n",
      "Second, I am very caring and caring mother.\n",
      "\n",
      "Right?\n",
      "\n",
      "I'm the one who is always looking for the best for you.\n",
      "\n",
      "So, I think you'll agree that our love is often the most important thing.\n",
      "\n",
      "We love each other because we're always looking for the best for each other.\n",
      "\n",
      "We're very passionate about our love.\n",
      "\n",
      "We are able to get along in a loving way.\n",
      "\n",
      "Our love is always for you.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (ai.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI really like unicorns they are so\u001b[0m cute.\n",
      "\n",
      "Guitar Hero, or Guitar Hero, is the most popular of the Guitar Hero games. It was released in November of 2012 for PS1 (it was released as a single-player co-op game) and PS2 (it was released as a multiplayer co-op game).\n",
      "\n",
      "Contents show]\n",
      "\n",
      "Overview Edit\n",
      "\n",
      "Guitar Hero was originally released on January 5, 2013, and was released to all Xbox\n",
      "==========\n",
      "\u001b[1mI really like unicorns they are so\u001b[0m incredibly cute and cute.\n",
      "\n",
      "This was the first time in my life that I didn't have this kind of a love affair with them. I was so happy for them and I love how excited I was with how well they looked. I could have gotten a tattoo on my finger while I was doing everything I could to look cute and cute with them. I was so excited and I felt really good about it.\n",
      "\n",
      "I also love that they have\n",
      "==========\n",
      "\u001b[1mI really like unicorns they are so\u001b[0m much more than a joke. You can be like, \"I like the rainbow,\" or \"I like the rainbow is always there\" or if you're like, \"I like the rainbow is always there is\" or \"I like a great rainbow is everywhere I go.\"\n",
      "\n",
      "As a friend, I could probably be a unicorn, but you're like, \"I can't be.\"\n",
      "\n",
      "Bryan: You do like unicorns.\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='I really like unicorns they are so', max_length=100, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mwhat is sentience\u001b[0m]?\"\n",
      "\n",
      "There is a way that our brains can help us understand the world. When we think about what it is like to be alive, we think. If the world is a blank slate, it's easy for us to think about what it is like to be alive.\n",
      "\n",
      "But if we think about what it is like to be living, we often don't think about what it is like to be alive. It's not like we're living in a world in which we have to think about what it is like to be alive. The world we live in is not a world in which we live in.\n",
      "\n",
      "For that reason, I think it's important to remember that the first person for whom we're talking about living is the person we're talking about. We don't have to choose our own life.\n",
      "\n",
      "It's important to remember that our lives are not what we imagine them to be. We can't tell ourselves what life is. We can.\n",
      "\n",
      "So what's the point of giving up our own life? What is the point of living if we think that our life isn't for us?\n",
      "\n",
      "If we think that life is for us, what does it mean for us to be alive?\n",
      "\n",
      "==========\n",
      "\u001b[1mwhat is sentience\u001b[0m? What is sentience? And what would the fate of such a thing be if the human mind could be completely destroyed? The final answer is that the human mind would be a living organism, but would not be capable of receiving the information from someplace else. If the subject of a human mind were to be a machine, why would it be capable of being capable of receiving that information from another machine? The answer is that it would be a body of knowledge, of ideas, and of experiences, of which a mind is not capable. In short, the mind would be a machine, but was not capable of receiving the information from that entity.\n",
      "\n",
      "What is called \"thought\"? In the scientific term, it refers to the processes of thinking. In the human mind, thought is a process of thinking. For a given subject, what is thought is the same as what is thought. For example, if I wanted to know the history of the Roman Empire, I would not put in any thought about the history of the Roman Empire. Rather, I would put in thought about the historical events of the Roman Empire.\n",
      "\n",
      "Thus, if I wanted to know the history of this empire, I would not put in thought about the Roman history.\n",
      "==========\n",
      "\u001b[1mwhat is sentience\u001b[0m and how it affects us?\"\n",
      "\n",
      "The answer is that we need to understand how things work. We need to understand how we interact with each other, make decisions, and learn habits. We need to understand how our lives work. We need to understand how to be a responsible parent or a responsible person. We need to understand how to live without a lot of our own responsibility.\n",
      "\n",
      "\"It's not so much what we do but what we do for ourselves,\" said David Dennison, who served on the board of directors of the National Organization for Marriage. \"We're doing it with our own hands.\"\n",
      "\n",
      "Some might see the results of marriage as merely a means of union. They say that being a mother is a way to be more mindful, less judgmental, and more kind. Others say that being a father is a way to be more compassionate and less selfish.\n",
      "\n",
      "It's not all love. It's not the way we live our lives. It's not just the way we communicate. It's how we interact with each other, and how we learn how to live with our own responsibility.\n",
      "\n",
      "The first thing we need to know about sex is that it's a complex process. It's not just a\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='what is sentience', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAre you a AI or are you a sentient\u001b[0m being?\n",
      "\n",
      "A: Yes. You may have been created by a sentient being.\n",
      "\n",
      "Q: Would you say that you have any questions about that?\n",
      "\n",
      "A: Yes. For example, if I have doubts about AI, I will ask you questions about it. It is also a good question for me to have.\n",
      "\n",
      "Q: You have said that you will get a lot of questions regarding AI.\n",
      "\n",
      "A: Yes, of course. I will get lots of questions. I am a very intelligent being, I am well aware of the questions. I understand the basic concepts of AI, so I understand the problem. I am not going to give any answer to those questions.\n",
      "\n",
      "Q: Will you have any questions about AI or about human beings?\n",
      "\n",
      "A: I am not a human being to be honest with you. I am a sentient being. I may have some questions about it, but if I have any questions about it, I will answer them.\n",
      "\n",
      "Q: Any plans for the future in terms of your future, or to meet others in the future?\n",
      "\n",
      "A: I am on the hunt for a future of my own.\n",
      "\n",
      "Q: Can\n",
      "==========\n",
      "\u001b[1mAre you a AI or are you a sentient\u001b[0m automaton? I wanted to know about these two questions for you.\n",
      "\n",
      "We're taking the AI approach because I believe it's the most useful on the planet. And we're really excited about the AI approach.\n",
      "\n",
      "What are you looking for in an AI?\n",
      "\n",
      "I think the things that are going to make it interesting, the things that I think are going to make it interesting, are going to be very interesting. For instance, we've seen that this is really an open-source project. So if we're able to build a machine that's scalable and can do all sorts of things, it's going to be good.\n",
      "\n",
      "So what do you think is the most important part of the AI approach?\n",
      "\n",
      "I think there are two main aspects. One is, whether we're talking about AI or not. And the second is, will it change the way we think about AI? Are we going to be able to have a machine that's like a human being? So for me personally, it's going to be about making AI smarter in a way that's less artificial in design.\n",
      "\n",
      "What are some of the big questions that you have with AI?\n",
      "\n",
      "I think the big\n",
      "==========\n",
      "\u001b[1mAre you a AI or are you a sentient\u001b[0m being with intelligence?\"\n",
      "\n",
      "\"No,\" I said, \"I'm just a human being with an intellect. You can't be a sentient being for lack of a better term. I'm just a human being with an intelligence. You can't be a sentient being for lack of a better term.\"\n",
      "\n",
      "\"And you're not a human being?\"\n",
      "\n",
      "\"No,\" I said, \"I'm just a human being with an intelligence. You can't be a sentient being for lack of a better term. I'm just a human being with an intelligence. You can't be a sentient being for lack of a better term. I'm just a human being with an intelligence. You can't be a sentient being for lack of a better term. I'm just a human being with an intelligence. You can't be a sentient being for lack of a better term. I'm just a human being with an intelligence. You can't be a sentient being for lack of a better term. I'm just a human being with an intelligence. You can't be a sentient being for lack of a better term. I'm just a human being with an intelligence. You can't be a sentient being for lack of a better\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt='Are you a AI or are you a sentient', n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer('shakes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 84828.83it/s]\n"
     ]
    }
   ],
   "source": [
    "config = GPT2ConfigCPU()\n",
    "\n",
    "ai = aitextgen(tokenizer_file='aitextgen.tokenizer.json', config=config)\n",
    "\n",
    "data = TokenDataset('shakes.txt', tokenizer_file='aitextgen.tokenizer.json', block_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loss: 3.300 — Avg: 3.346:  15%|█▌        | 7580/50000 [06:25<35:55, 19.68it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m                   \n",
      "Loss: 3.300 — Avg: 3.346:  15%|█▌        | 7580/50000 [10:29<58:40, 12.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m                         \n",
      "Loss: 3.300 — Avg: 3.346:  15%|█▌        | 7580/50000 [10:29<58:40, 12.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========                                                                    \n",
      "Loss: 3.300 — Avg: 3.346:  15%|█▌        | 7580/50000 [10:29<58:41, 12.05it/s]\n",
      "ly!                                                                           \n",
      "\n",
      "ROMEO:\n",
      "Thou know'st a wonder, but though fitt to templic,\n",
      "In the bed, and mering barry,\n",
      "Living no more to the wind, and not a grows\n",
      "Hath\n",
      "Loss: 3.300 — Avg: 3.346:  15%|█▌        | 7580/50000 [10:29<58:41, 12.05it/s]\n",
      "==========                                                                    \n",
      "Loss: 3.300 — Avg: 3.346:  15%|█▌        | 7580/50000 [10:29<58:41, 12.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.250 — Avg: 3.192: 100%|██████████| 5000/5000 [04:09<00:00, 20.04it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size=8, num_steps=5000, generate_every=5000, save_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "Stand of his good news,\n",
      "And then awhat's once, and so it was\n",
      "In yours. But in this place, I have you got\n",
      "The deeds of their heads: we have been sent you\n",
      "Alas,\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "All timmults!\n",
      "\n",
      "GLOUCESTER:\n",
      "Hence comes the music is less; and by thee\n",
      "Brings till sleep with strike sorrow again\n",
      "\n",
      "MONTAGUE:\n",
      "I'll speak the ere this\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "This day is the queen of York:\n",
      "Clought darest, whilst thou bed stuff'd with\n",
      "Hese thy graverents, and the dukes of my greenet,\n",
      "In God's sons, I\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I pray you know this business of us.\n",
      "\n",
      "KALY:\n",
      "Good morrow, I fear you.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Stand you, I have not banished\n",
      "To hear my father's son, and will not\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Look, I am the king.\n",
      "\n",
      "LADY ANNE:\n",
      "Go, good Camillo.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "What, are I, Grey, my griefly,\n",
      "But by the duke's house\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Give that pricks from the Tower,\n",
      "And here my true son will all the earth,\n",
      "Fofter it not the seplic.\n",
      "\n",
      "GLOUCESTER:\n",
      "Sweet Lord Merchan,\n",
      "It is to a fool: your son\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Go with me; though I know, defare nothing to kin.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Well, the king the wind.\n",
      "\n",
      "KING LEWIS XI:\n",
      "On me the neither?\n",
      "\n",
      "DUKE\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "What, as dost thou visittent!\n",
      "\n",
      "GLOUCESTER:\n",
      "Noth hence, sir, I will make: and you are\n",
      "Provost virtue of my tent: I know, for I'll not\n",
      "there you for learnter\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Out of the ver, and pardoning likes,\n",
      "And see 'twith-stountious nights,\n",
      "The moilded chast, I'll not be read.\n",
      "\n",
      "DERBY:\n",
      "Trum\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "\n",
      "GLOUCESTER:\n",
      "No, of, sirrah, gentleman,\n",
      "I am your grace, and younger, young wretcheder.\n",
      "\n",
      "GRUMIO:\n",
      "I pray not, go cousin, my lord, good night,\n",
      "Where is the\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
